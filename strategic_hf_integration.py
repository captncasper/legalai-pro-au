#!/usr/bin/env python3
"""
Strategic Hugging Face Corpus Integration
Maximizes intelligence without overwhelming resources
"""

from datasets import load_dataset
import pickle
from typing import Dict, List
import json
from collections import defaultdict
import re
from tqdm import tqdm
import numpy as np

class StrategicHFIntegration:
    def __init__(self):
        self.priority_jurisdictions = ['federal', 'high_court', 'nsw']
        self.high_value_patterns = {
            'landmark_cases': [
                'leading case', 'high court', 'full bench',
                'principle established', 'overturned', 'landmark'
            ],
            'recent_precedents': [
                '2023', '2024', '2022', 'recent authority'
            ],
            'employment_focus': [
                'unfair dismissal', 'fair work', 'discrimination',
                'workplace', 'employment', 'industrial relations'
            ]
        }
    
    def smart_stream_and_extract(self, target_additions: int = 40000):
        """Stream HF corpus and extract only high-value additions"""
        
        print("ðŸŒŠ Streaming Open Australian Legal Corpus...")
        
        # Stream to save memory
        dataset = load_dataset(
            'umarbutler/open-australian-legal-corpus',
            split='corpus',
            streaming=True
        )
        
        extracted_data = {
            'high_value_docs': [],
            'precedent_network': defaultdict(list),
            'outcome_patterns': defaultdict(int),
            'settlement_database': [],
            'legislative_updates': []
        }
        
        docs_processed = 0
        docs_selected = 0
        
        for doc in tqdm(dataset, desc="Processing", total=target_additions * 3):
            docs_processed += 1
            
            # Stop if we have enough
            if docs_selected >= target_additions:
                break
            
            # Extract and score
            score, intelligence = self._extract_document_intelligence(doc)
            
            # Only keep high-value documents
            if score > 50:  # High threshold
                # Don't store full text - just intelligence
                compressed_doc = {
                    'citation': doc.get('citation', ''),
                    'jurisdiction': doc.get('jurisdiction', ''),
                    'year': intelligence.get('year'),
                    'key_points': intelligence.get('key_points', []),
                    'outcome': intelligence.get('outcome'),
                    'amounts': intelligence.get('amounts', []),
                    'precedents_cited': intelligence.get('precedents', []),
                    'text_sample': doc.get('text', '')[:500]  # Just sample
                }
                
                extracted_data['high_value_docs'].append(compressed_doc)
                docs_selected += 1
                
                # Update pattern databases
                if intelligence.get('outcome'):
                    extracted_data['outcome_patterns'][intelligence['outcome']] += 1
                
                # Build precedent network
                for precedent in intelligence.get('precedents', []):
                    extracted_data['precedent_network'][precedent].append(
                        doc.get('citation', '')
                    )
                
                # Extract settlements
                if intelligence.get('amounts'):
                    extracted_data['settlement_database'].extend(
                        intelligence['amounts']
                    )
        
        print(f"\nâœ… Processed {docs_processed} documents")
        print(f"âœ… Selected {docs_selected} high-value documents")
        print(f"âœ… Extracted {len(extracted_data['settlement_database'])} settlement amounts")
        print(f"âœ… Built precedent network with {len(extracted_data['precedent_network'])} nodes")
        
        return extracted_data
    
    def _extract_document_intelligence(self, doc: Dict) -> tuple:
        """Extract intelligence without storing full text"""
        
        text = doc.get('text', '')
        metadata = doc.get('metadata', {})
        
        score = 0
        intelligence = {}
        
        # Jurisdiction scoring
        jurisdiction = metadata.get('jurisdiction', '').lower()
        if any(pj in jurisdiction for pj in self.priority_jurisdictions):
            score += 20
        
        # Year extraction and scoring
        year_match = re.search(r'20(\d{2})', text[:1000])
        if year_match:
            year = 2000 + int(year_match.group(1))
            intelligence['year'] = year
            if year >= 2020:
                score += 30
            elif year >= 2015:
                score += 15
        
        # Pattern matching for high-value content
        text_lower = text.lower()
        for category, patterns in self.high_value_patterns.items():
            matches = sum(1 for p in patterns if p in text_lower)
            if matches > 0:
                score += matches * 10
        
        # Extract key intelligence
        intelligence['key_points'] = self._extract_key_points(text)
        intelligence['outcome'] = self._extract_outcome(text_lower)
        intelligence['amounts'] = self._extract_amounts(text)
        intelligence['precedents'] = self._extract_precedents(text)
        
        # Bonus for employment law
        if 'fair work' in text_lower or 'employment' in text_lower:
            score += 25
        
        return score, intelligence
    
    def _extract_key_points(self, text: str) -> List[str]:
        """Extract key legal points"""
        key_points = []
        
        # Look for numbered points or principles
        principle_match = re.findall(
            r'(?:principle|held|established|determined)[\s:]+(.{20,200})',
            text,
            re.IGNORECASE
        )
        
        key_points.extend(principle_match[:3])
        return key_points
    
    def _extract_outcome(self, text: str) -> str:
        """Determine case outcome"""
        if 'application granted' in text or 'appeal allowed' in text:
            return 'applicant_won'
        elif 'application dismissed' in text or 'appeal dismissed' in text:
            return 'applicant_lost'
        elif 'settled' in text or 'consent orders' in text:
            return 'settled'
        return 'unknown'
    
    def _extract_amounts(self, text: str) -> List[int]:
        """Extract monetary amounts"""
        amounts = []
        
        # Find compensation amounts
        amount_matches = re.findall(
            r'\$(\d{1,3}(?:,\d{3})*)',
            text
        )
        
        for match in amount_matches:
            try:
                amount = int(match.replace(',', ''))
                if 5000 < amount < 500000:  # Reasonable range
                    amounts.append(amount)
            except:
                continue
        
        return amounts[:5]  # Limit to avoid false positives
    
    def _extract_precedents(self, text: str) -> List[str]:
        """Extract cited cases"""
        precedents = []
        
        # Pattern for Australian case citations
        citation_pattern = r'(?:[A-Z]\w+\s+v\s+[A-Z]\w+.*?\[\d{4}\].*?(?:HCA|FCA|FCAFC|FWC)\s*\d+)'
        
        matches = re.findall(citation_pattern, text)
        precedents.extend(matches[:10])  # Top 10
        
        return precedents

class IntelligentModelIntegration:
    """Integrate HF pre-trained models intelligently"""
    
    @staticmethod
    def download_embeddings_only():
        """Download just the embeddings from the legal LLM"""
        
        print("ðŸ¤– Downloading legal embeddings...")
        
        # We can use the embeddings without the full model
        from transformers import AutoModel, AutoTokenizer
        
        # Download tokenizer (small)
        tokenizer = AutoTokenizer.from_pretrained(
            'umarbutler/open-australian-legal-corpus-gpt2'
        )
        
        # For embeddings, we just need the tokenizer
        # The full model is 1.5GB - too big for Codespaces
        
        return {
            'tokenizer': tokenizer,
            'vocab_size': tokenizer.vocab_size,
            'legal_tokens': [t for t in tokenizer.get_vocab() if 'legal' in t.lower()]
        }

class HybridIntelligenceBuilder:
    """Combine your corpus with HF intelligence"""
    
    def build_hybrid_system(
        self,
        current_corpus_intel: Dict,
        hf_extracted_intel: Dict
    ) -> Dict:
        """Merge intelligences for maximum power"""
        
        print("ðŸ”€ Building hybrid intelligence system...")
        
        hybrid_intel = {
            'pattern_library': self._merge_patterns(
                current_corpus_intel.get('winning_patterns', {}),
                hf_extracted_intel
            ),
            'settlement_intelligence': self._enhance_settlement_data(
                current_corpus_intel.get('settlement_intelligence', {}),
                hf_extracted_intel.get('settlement_database', [])
            ),
            'precedent_strength': self._calculate_precedent_strength(
                hf_extracted_intel.get('precedent_network', {})
            ),
            'outcome_predictor': self._build_outcome_predictor(
                current_corpus_intel,
                hf_extracted_intel.get('outcome_patterns', {})
            )
        }
        
        return hybrid_intel
    
    def _merge_patterns(self, current_patterns: Dict, hf_intel: Dict) -> Dict:
        """Merge pattern knowledge"""
        merged = current_patterns.copy()
        
        # Enhance with HF patterns
        hf_docs = hf_intel.get('high_value_docs', [])
        
        # Extract patterns from HF samples
        pattern_counts = defaultdict(int)
        for doc in hf_docs:
            for point in doc.get('key_points', []):
                if 'no warning' in point.lower():
                    pattern_counts['no_warning'] += 1
                if 'long service' in point.lower():
                    pattern_counts['long_service'] += 1
        
        # Update win rates with larger sample
        for pattern, count in pattern_counts.items():
            if pattern in merged:
                # Weighted average with HF data
                merged[pattern]['occurrences'] += count
                merged[pattern]['confidence'] = 'VERY_HIGH'
        
        return merged
    
    def _enhance_settlement_data(
        self,
        current_settlements: Dict,
        hf_settlements: List[int]
    ) -> Dict:
        """Enhance settlement intelligence"""
        
        if not hf_settlements:
            return current_settlements
        
        # Combine all settlements
        all_settlements = []
        
        # Add current
        if current_settlements.get('percentiles'):
            # Estimate original values
            all_settlements.extend([current_settlements['median']] * 100)
        
        # Add HF settlements
        all_settlements.extend(hf_settlements)
        
        # Recalculate with larger sample
        return {
            'count': len(all_settlements),
            'average': np.mean(all_settlements),
            'median': np.median(all_settlements),
            'percentiles': {
                '10th': np.percentile(all_settlements, 10),
                '25th': np.percentile(all_settlements, 25),
                '50th': np.percentile(all_settlements, 50),
                '75th': np.percentile(all_settlements, 75),
                '90th': np.percentile(all_settlements, 90),
                '95th': np.percentile(all_settlements, 95)
            },
            'confidence': 'HIGH' if len(all_settlements) > 1000 else 'MEDIUM'
        }
    
    def _calculate_precedent_strength(self, precedent_network: Dict) -> Dict:
        """Calculate most influential precedents"""
        
        # Count citations
        citation_counts = defaultdict(int)
        for cited_by_list in precedent_network.values():
            for case in cited_by_list:
                citation_counts[case] += 1
        
        # Get top precedents
        top_precedents = sorted(
            citation_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:20]
        
        return {
            'most_cited': top_precedents,
            'network_size': len(precedent_network),
            'average_citations': np.mean(list(citation_counts.values())) if citation_counts else 0
        }
    
    def _build_outcome_predictor(
        self,
        current_intel: Dict,
        outcome_patterns: Dict
    ) -> Dict:
        """Build enhanced outcome predictor"""
        
        total_cases = sum(outcome_patterns.values())
        
        if total_cases == 0:
            return current_intel
        
        win_rate = outcome_patterns.get('applicant_won', 0) / total_cases
        
        return {
            'base_win_rate': win_rate,
            'outcome_distribution': {
                k: v/total_cases for k, v in outcome_patterns.items()
            },
            'sample_size': total_cases,
            'confidence': 'HIGH' if total_cases > 1000 else 'MEDIUM'
        }

if __name__ == "__main__":
    print("ðŸ§  STRATEGIC HUGGING FACE INTEGRATION")
    print("=" * 60)
    
    # Step 1: Extract from HF corpus
    hf_integrator = StrategicHFIntegration()
    hf_intelligence = hf_integrator.smart_stream_and_extract(
        target_additions=40000  # Add 40k best documents
    )
    
    # Save extracted intelligence
    with open('hf_extracted_intelligence.json', 'w') as f:
        json.dump(hf_intelligence, f, indent=2)
    print("âœ… Saved HF intelligence to hf_extracted_intelligence.json")
    
    # Step 2: Download legal embeddings
    model_integration = IntelligentModelIntegration()
    embeddings_info = model_integration.download_embeddings_only()
    print(f"âœ… Downloaded legal tokenizer with {embeddings_info['vocab_size']} tokens")
    
    # Step 3: Build hybrid system
    # Load your current intelligence
    try:
        with open('corpus_intelligence.json', 'r') as f:
            current_intel = json.load(f)
    except:
        current_intel = {}
    
    hybrid_builder = HybridIntelligenceBuilder()
    hybrid_intelligence = hybrid_builder.build_hybrid_system(
        current_intel,
        hf_intelligence
    )
    
    # Save hybrid intelligence
    with open('hybrid_intelligence.json', 'w') as f:
        json.dump(hybrid_intelligence, f, indent=2)
    
    print("\nâœ… HYBRID INTELLIGENCE READY!")
    print(f"ðŸ“Š Settlement data points: {hybrid_intelligence['settlement_intelligence']['count']}")
    print(f"ðŸ”— Precedent network size: {hybrid_intelligence['precedent_strength']['network_size']}")
    print(f"ðŸŽ¯ Enhanced patterns: {len(hybrid_intelligence['pattern_library'])}")
